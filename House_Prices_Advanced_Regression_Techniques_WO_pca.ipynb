{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House Prices: Advanced Regression Techniques_without_pca.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ibAhUobipOZ"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF9aiV7ci0j-"
      },
      "source": [
        "house_train_data = pd.read_csv('/content/train hp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFGU3WxUjJGF"
      },
      "source": [
        "house_train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a51UTHCbko0L"
      },
      "source": [
        "#house_test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIm1Xh7atqgw"
      },
      "source": [
        "house_train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNp8yt0ttx6i"
      },
      "source": [
        "print(house_train_data.columns[house_train_data.isna().any()].tolist())\n",
        "len(house_train_data.columns[house_train_data.isna().any()].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3WHtNQ8uBVs"
      },
      "source": [
        "features = [x for x in house_train_data.columns if x not in ['SalePrice']]\n",
        "X = house_train_data[features]\n",
        "y = house_train_data['SalePrice']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmUxGAAp6E2M"
      },
      "source": [
        "corr_matrix = house_train_data.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KECqokmY6VOs"
      },
      "source": [
        "corr_matrix[\"Id\"].sort_values(ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7b1o5Cf6YZ0"
      },
      "source": [
        "from pandas.plotting import scatter_matrix \n",
        "attributes = [\"Id\",\"PoolArea\",\"BedroomAbvGr\",\"TotRmsAbvGrd\",\"MoSold\",\"GarageArea\",       \n",
        "\"GarageCars\",\"OverallCond\",\"MSSubClass\",\"1stFlrSF\",\"GrLivArea\",\"HalfBath\",         \n",
        "\"2ndFlrSF\",\"FullBath\",\"KitchenAbvGr\",\"EnclosedPorch\",\"BsmtFullBath\",\"ScreenPorch\",\"YrSold\",           \n",
        "\"GarageYrBlt\",\"OpenPorchSF\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"MiscVal\",         \n",
        "\"BsmtUnfSF\",\"LotFrontage\",\"YearBuilt\",\"TotalBsmtSF\",\"Fireplaces\",\"BsmtHalfBath\",    \n",
        "\"SalePrice\",\"YearRemodAdd\",\"OverallQual\",\"WoodDeckSF\",\"LotArea\",\"LowQualFinSF\",    \n",
        "\"3SsnPorch\",\"MasVnrArea\"]\n",
        "scatter_matrix(house_train_data[attributes], figsize = (100,100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ckJQFE_uROS"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
        "\n",
        "numerical_cols = [cname for cname in X_train.columns if \n",
        "                X_train[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "categorical_cols = [cname for cname in X_train.columns if\n",
        "                    X_train[cname].nunique() < 10 and \n",
        "                    X_train[cname].dtype == \"object\"]\n",
        "\n",
        "\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE_nnGF8uvvD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_model = DecisionTreeRegressor(random_state=0)\n",
        "\n",
        "tree_clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('tree_model', tree_model)\n",
        "                     ])\n",
        "\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "tree_preds = tree_clf.predict(X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_valid, tree_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL531LZUu_M3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "random_model = RandomForestRegressor(random_state=0)\n",
        "\n",
        "random_clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('random_model', random_model)\n",
        "                     ])\n",
        "\n",
        "random_clf.fit(X_train, y_train)\n",
        "\n",
        "random_clf.fit(X_train, y_train)\n",
        "\n",
        "random_preds = random_clf.predict(X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_valid, random_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Vxyzy4vPdF"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.07, random_state=0)\n",
        "\n",
        "xgb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('xgb_model', xgb_model)\n",
        "                     ])\n",
        "\n",
        "xgb_clf.fit(X_train, y_train, xgb_model__verbose=False)\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "xgb_preds = xgb_clf.predict(X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_valid, xgb_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI4gcl8NvXDI"
      },
      "source": [
        "'''from sklearn.model_selection import GridSearchCV\n",
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "grid = GridSearchCV(xgb_model, param_grid=params, n_jobs=4, cv=5, verbose=3 )\n",
        "grid.fit(param_X, y)\n",
        "print('\\n All results:')\n",
        "print(grid.cv_results_)\n",
        "print('\\n Best estimator:')\n",
        "print(grid.best_estimator_)\n",
        "print('\\n Best score:')\n",
        "print(grid.best_score_ * 2 - 1)\n",
        "print('\\n Best parameters:')\n",
        "print(grid.best_params_)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxhzbh0Xxp-u"
      },
      "source": [
        "hp_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=0.6, gamma=0.5, gpu_id=1,\n",
        "             importance_type='gain', interaction_constraints='',\n",
        "             learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
        "             min_child_weight=1, monotone_constraints=1,\n",
        "             n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
        "             tree_method='exact', validate_parameters=1, verbosity=0)\n",
        "\n",
        "hp_clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('hp_model', hp_model)\n",
        "                     ])\n",
        "\n",
        "hp_clf.fit(X_train, y_train, hp_model__verbose=0)\n",
        "\n",
        "hp_preds = hp_clf.predict(X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_valid, hp_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnMVdmbyOK5"
      },
      "source": [
        "X.columns.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSNoPsEo0P6u"
      },
      "source": [
        "print(X['YearBuilt'].head())\n",
        "print(X['YearRemodAdd'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "396p9VBn0TwB"
      },
      "source": [
        "print(X['LotArea'].head())\n",
        "print(X['LotFrontage'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21FYrANk0X5W"
      },
      "source": [
        "print(set(X['LandSlope']))\n",
        "print(set(X['LandContour']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K20mUeeC0bNM"
      },
      "source": [
        "print(set(X['YrSold']))\n",
        "print(set(X['MoSold']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4neN-Toj0fuP"
      },
      "source": [
        "print(set(X['Condition1']))\n",
        "print(set(X['Condition2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UulSf60B0i4u"
      },
      "source": [
        "print(set(X['ExterQual']))\n",
        "print(set(X['ExterCond']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc4tlVd70od8"
      },
      "source": [
        "X_feat_eng = X.copy()\n",
        "X_feat_eng['years_since_update'] = X_feat_eng['YearRemodAdd'] - X_feat_eng['YearBuilt']\n",
        "X_feat_eng['geometry'] = X_feat_eng['LotArea'] / X_feat_eng['LotFrontage']\n",
        "X_feat_eng['land_topology'] = X_feat_eng['LandSlope'] + '_' + X_feat_eng['LandContour']\n",
        "\n",
        "feature_numerical_cols = [cname for cname in X_feat_eng.columns if \n",
        "                X_feat_eng[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "feature_categorical_cols = [cname for cname in X_feat_eng.columns if\n",
        "                    X_feat_eng[cname].nunique() < 13 and \n",
        "                    X_feat_eng[cname].dtype == \"object\"]\n",
        "\n",
        "\n",
        "feature_numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "feature_categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "feature_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', feature_numerical_transformer, feature_numerical_cols),\n",
        "        ('cat', feature_categorical_transformer, feature_categorical_cols)\n",
        "])\n",
        "\n",
        "feature_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=0.6, gamma=0.0, gpu_id=1,\n",
        "             importance_type='gain', interaction_constraints='',\n",
        "             learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
        "             min_child_weight=0.0, monotone_constraints='1',\n",
        "             n_estimators=1250, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
        "             tree_method='exact', validate_parameters=1, verbosity=0)\n",
        "feature_clf = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n",
        "                      ('feature_model', feature_model)\n",
        "                     ])\n",
        "\n",
        "feature_X_train, feature_X_valid, feature_y_train, feature_y_valid = train_test_split(X_feat_eng, y, random_state=0)\n",
        "\n",
        "feature_clf.fit(feature_X_train, feature_y_train, feature_model__verbose=False)\n",
        "\n",
        "feature_preds = feature_clf.predict(feature_X_valid)\n",
        "\n",
        "print('MAE:', mean_absolute_error(feature_y_valid, feature_preds))\n",
        "#MAE: 15483.647185359589"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxLeBU3o09ya"
      },
      "source": [
        "X_final = X.copy()\n",
        "X_final['years_since_update'] = X_final['YearRemodAdd'] - X_final['YearBuilt']\n",
        "X_final['geometry'] = X_final['LotArea'] / X_final['LotFrontage']\n",
        "X_final['land_topology'] = X_final['LandSlope'] + '_' + X_final['LandContour']\n",
        "\n",
        "final_numerical_cols = [cname for cname in X_final.columns if \n",
        "                X_final[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "final_categorical_cols = [cname for cname in X_final.columns if\n",
        "                    X_final[cname].nunique() < 13 and \n",
        "                    X_final[cname].dtype == \"object\"]\n",
        "\n",
        "\n",
        "final_numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "final_categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "final_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', final_numerical_transformer, final_numerical_cols),\n",
        "        ('cat', final_categorical_transformer, final_categorical_cols)\n",
        "])\n",
        "\n",
        "final_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=0.6, gamma=0.0, gpu_id=1,\n",
        "             importance_type='gain', interaction_constraints='',\n",
        "             learning_rate=0.07, max_delta_step=0, max_depth=4,\n",
        "             min_child_weight=0, monotone_constraints='1',\n",
        "             n_estimators=1250, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
        "             tree_method='exact', validate_parameters=1, verbosity=0)\n",
        "\n",
        "final_clf = Pipeline(steps=[('final_preprocessor', final_preprocessor),\n",
        "                      ('final_model', final_model)\n",
        "                     ])\n",
        "\n",
        "final_clf.fit(X_final, y, final_model__verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8LVVlxA1k_A"
      },
      "source": [
        "X_test = pd.read_csv('/content/test hp.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0H1rFpF3vZT"
      },
      "source": [
        "X_test['years_since_update'] = X_test['YearRemodAdd'] - X_test['YearBuilt']\n",
        "X_test['geometry'] = X_test['LotArea'] / X_test['LotFrontage']\n",
        "X_test['land_topology'] = X_test['LandSlope'] + '_' + X_test['LandContour']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h55aMcRn3w6n"
      },
      "source": [
        "preds = final_clf.predict(X_test)\n",
        "output = pd.DataFrame({'Id': X_test.Id,\n",
        "                       'SalePrice': preds})\n",
        "output.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXgwTHxmQysx"
      },
      "source": [
        "# **Ans of 4**\n",
        "\n",
        "Ensemble modeling is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.\n",
        "\n",
        "Ensemble learning is a broad topic and is only confined by your own imagination. For the purpose of this article, I will cover the basic concepts and ideas of ensemble modeling. This should be enough for you to start building ensembles at your own end. As usual, we have tried to keep things as simple as possible.\n",
        "\n",
        "The difference between Bagging and Boosting:\n",
        "\n",
        "1) While they are built independently for Bagging, Boosting tries to add new models that do well where previous models fail.\n",
        "\n",
        "2) Only Boosting determines weights for the data to tip the scales in favor of the most difficult cases.\n",
        "\n",
        "3) It is an equally weighted average for Bagging and a weighted average for Boosting, more weight to those with better performance on training data.\n",
        "\n",
        "4) Only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting problem, while Boosting can increase it.\n",
        "\n",
        "\n"
      ]
    }
  ]
}